import os
from dataclasses import dataclass
from typing import Iterable, List, Literal, Self, Tuple, TypeVar

import torch
from conllup.conllup import _featuresConllToJson, sentenceJson_T
from torch import Tensor, tensor
from torch.utils.data import Dataset
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast

from .annotation_schema import CONLLU_BLANK, DUMMY_ID, AnnotationSchema_T
from .lemma_script_utils import apply_lemma_rule

CopyOption = Literal["NONE", "EXISTING", "ALL"]


@dataclass
class PartialPredictionConfig:
    keep_upos: CopyOption = "NONE"
    keep_xpos: CopyOption = "NONE"
    keep_heads: CopyOption = "NONE"
    keep_deprels: CopyOption = "NONE"
    keep_feats: CopyOption = "NONE"
    keep_miscs: CopyOption = "NONE"
    keep_lemmas: CopyOption = "NONE"


# TODO: we've split prediction and training data classes so that we could save memory
# when we didn't need to keep the training data around. But we ended up not needing the
# prediction classes because memory was not an issue. We should probably merge them
# again.


# Words and tokens are not the same! A word can be composed of multiple subword tokens.
# In the classes below, we work with (smallest to largest) tokens and words contained
# in sequences (or sentences) and batches of sequences. We'll use T for the number of
# tokens in a sequence (including added special tokens), W for the number of words in
# a sequence, and B for the number of sequences in a batch.
@dataclass
class SequencePrediction_T:
    """Index in the dataset"""

    idx: int
    """the ConllU data for the sentence"""
    sentence_json: sentenceJson_T

    """The token ids of the sequence, prepended with a CLS token and appended with a SEP
    token as required by BERT models. Size is (T)."""
    sequence_token_ids: List[int]
    """True if sequence token begins a new word, 0 otherwise. Size is (T)."""
    tok_starts_word: List[bool]

    """Maps word index + 1 to the index in the sequence_token_ids where the word begins.
    Size is (W)."""
    idx_converter: List[int]
    """The number of tokens in each input word. Size is (W)."""
    tokens_len: List[int]


Sequence_T = TypeVar("Sequence_T", bound=SequencePrediction_T)


@dataclass
class SequencePredictionBatch_T:
    """Except for attn_masks and max_sentence_legth, each field contains all of the
    tensors of the corresponding field in SequencePrediction_T for each sequence in the
    batch. See that class for more details. Sizes are then (B, T or W) for each batched
    field."""

    idx: Tensor
    sequence_token_ids: Tensor
    # Tensor of shape [batch_size, max_seq_length] containing attention masks to be used
    # to avoid contribution of PAD tokens. Size is (B, T).
    # See https://huggingface.co/docs/transformers/glossary#attention-mask.
    attn_masks: Tensor

    tok_starts_word: Tensor
    idx_converter: Tensor

    # The maximum length of any sequence in the batch, determining the size of the
    # tensors representing sequences (shorter sequences are padded).
    max_sentence_length: int

    def to(self, device: torch.device) -> Self:
        tok_starts_word = self.tok_starts_word
        idx_converter = self.idx_converter
        return SequencePredictionBatch_T(
            idx=self.idx,
            sequence_token_ids=self.sequence_token_ids.to(device),
            attn_masks=self.attn_masks.to(device),
            tok_starts_word=tok_starts_word,
            idx_converter=idx_converter,
            max_sentence_length=self.max_sentence_length,
        )

    def pin_memory(self):
        self.idx = self.idx.pin_memory()
        self.sequence_token_ids = self.sequence_token_ids.pin_memory()
        self.attn_masks = self.attn_masks.pin_memory()
        self.tok_starts_word = self.tok_starts_word.pin_memory()
        self.idx_converter = self.idx_converter.pin_memory()
        return self


@dataclass
class SequenceTrainingBatch_T(SequencePredictionBatch_T):
    """Each field contains all of the tensors of the corresponding field in
    SequenceTraining_T for each sequence in the batch. See that class for more details.
    Sizes are then (B, T) for each batched field."""

    uposs: Tensor
    xposs: Tensor
    heads: Tensor
    deprels: Tensor
    feats: Tensor
    miscs: Tensor
    lemma_scripts: Tensor

    def __init__(
        self,
        pred_data: SequencePredictionBatch_T,
        uposs: Tensor,
        xposs: Tensor,
        heads: Tensor,
        deprels: Tensor,
        feats: Tensor,
        miscs: Tensor,
        lemma_scripts: Tensor,
    ):
        super().__init__(**pred_data.__dict__)
        self.uposs = uposs
        self.xposs = xposs
        self.heads = heads
        self.deprels = deprels
        self.feats = feats
        self.miscs = miscs
        self.lemma_scripts = lemma_scripts

    def to(self, device: torch.device):
        return SequenceTrainingBatch_T(
            pred_data=super().to(device),
            heads=self.heads.to(device),
            deprels=self.deprels.to(device),
            uposs=self.uposs.to(device),
            xposs=self.xposs.to(device),
            feats=self.feats.to(device),
            miscs=self.miscs.to(device),
            lemma_scripts=self.lemma_scripts.to(device),
        )

    def pin_memory(self):
        super().pin_memory()
        self.heads = self.heads.pin_memory()
        self.deprels = self.deprels.pin_memory()
        self.uposs = self.uposs.pin_memory()
        self.xposs = self.xposs.pin_memory()
        self.feats = self.feats.pin_memory()
        self.miscs = self.miscs.pin_memory()
        self.lemma_scripts = self.lemma_scripts.pin_memory()
        return self


class SequenceTraining_T(SequencePrediction_T):
    """Each list is size (T) and gives the ID of the class label for the corresponding
    token in the sequence, where tokens that do not begin a new word are given a dummy
    class value."""

    uposs: List[int]
    xposs: List[int]
    heads: List[int]
    deprels: List[int]
    feats: List[int]
    miscs: List[int]
    lemma_scripts: List[int]

    def __init__(
        self,
        pred_data: SequencePrediction_T,
        uposs: List[int],
        xposs: List[int],
        heads: List[int],
        deprels: List[int],
        feats: List[int],
        miscs: List[int],
        lemma_scripts: List[int],
    ):
        super().__init__(**pred_data.__dict__)
        self.uposs = uposs
        self.xposs = xposs
        self.heads = heads
        self.deprels = deprels
        self.feats = feats
        self.miscs = miscs
        self.lemma_scripts = lemma_scripts


class UDDataset(Dataset[SequenceTraining_T]):
    """Universal Dependency dataset tokenized and encoded for input to the
    BertForDeprel model."""

    def __init__(
        self,
        sentences: Iterable[sentenceJson_T],
        annotation_schema: AnnotationSchema_T,
        embedding_type: str,
        max_position_embeddings: int,
    ):
        self.annotation_schema = annotation_schema
        self.max_position_embeddings = max_position_embeddings

        # TODO: what's this for?
        os.environ["TOKENIZERS_PARALLELISM"] = "true"
        self.tokenizer: (
            PreTrainedTokenizer | PreTrainedTokenizerFast
        ) = AutoTokenizer.from_pretrained(embedding_type)

        if self.tokenizer.cls_token_id is None:
            raise Exception("CLS token not found in tokenizer")
        self.CLS_token_id = self.tokenizer.cls_token_id

        if self.tokenizer.sep_token_id is None:
            raise Exception("SEP token not found in tokenizer")
        self.SEP_token_id = self.tokenizer.sep_token_id

        if self.tokenizer.unk_token_id is None:
            raise Exception("UNK token not found in tokenizer")
        self.UNK_token_id = self.tokenizer.unk_token_id

        if self.tokenizer.pad_token_id is None:
            raise Exception("PAD token not found in tokenizer")
        self.PAD_token_id = self.tokenizer.pad_token_id

        # 0 (CLS), 2 (SEP), 3 (UNK), 1 (PAD)
        print(
            f"Special tokens are {self.CLS_token_id} (CLS), {self.SEP_token_id} (SEP), "
            f"{self.UNK_token_id} (UNK), {self.tokenizer.pad_token_id} (PAD)"
        )

        self._encode_conllu(sentences)

    def _encode_conllu(self, sentences: Iterable[sentenceJson_T]):
        self.sequences: List[SequenceTraining_T] = []
        valid_sentence_counter = 0
        for sentence_json in sentences:
            sequence = self._get_processed(sentence_json, valid_sentence_counter)
            # We save one spot each for CLS_token_id and SEP_token_id
            if len(sequence.sequence_token_ids) > self.max_position_embeddings - 2:
                print("Discarding sentence", len(sequence.sequence_token_ids))
                continue
            self.sequences.append(sequence)
            valid_sentence_counter += 1

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx: int) -> SequenceTraining_T:
        return self.sequences[idx]

    T = TypeVar("T")

    def _pad_list(self, list_to_pad: List[T], padding_value: T, maxlen: int):
        if len(list_to_pad) > maxlen:
            print(list_to_pad, len(list_to_pad))
            raise Exception(
                f"Sequence length (={len(list_to_pad)}) exceeds maximum allowed "
                f"length ({maxlen})"
            )

        return list_to_pad + [padding_value] * (maxlen - len(list_to_pad))

    def _get_input(self, sequence: sentenceJson_T, idx: int) -> SequencePrediction_T:
        sequence_ids = [self.CLS_token_id]
        tok_starts_word = [False]

        idx_converter = [0]
        tokens_len = [1]

        for input_token in sequence["treeJson"]["nodesJson"].values():
            if not isinstance(input_token["ID"], str):
                continue
            form = input_token["FORM"]
            # Note that we are simply ignoring un-recognized parts of the form; if all
            # parts are unrecognized, then we replace with UNK. TODO: we need
            # add_special_tokens to be False, but we still need UNK tokens
            token_ids = self.tokenizer.encode(form, add_special_tokens=False)
            if len(token_ids) == 0:
                print(
                    f"WARNING: Input token {input_token['ID']} ('{form}') of sentence "
                    f"{sequence['metaJson']['sent_id']} is not present in the tokenizer"
                    " vocabulary; using UNK instead."
                )
                token_ids = [self.UNK_token_id]
            idx_converter.append(len(sequence_ids))
            tokens_len.append(len(token_ids))

            sequence_ids += token_ids
            tok_starts_word += [True] + [False] * (len(token_ids) - 1)

        sequence_ids = sequence_ids + [self.SEP_token_id]
        return SequencePrediction_T(
            idx=idx,
            sentence_json=sequence,
            sequence_token_ids=sequence_ids,
            tok_starts_word=tok_starts_word,
            idx_converter=idx_converter,
            tokens_len=tokens_len,
        )

    def _get_output(
        self, sequence: sentenceJson_T, input: SequencePrediction_T
    ) -> SequenceTraining_T:
        # initialize with dummy values for the leading CLS token
        uposs = [DUMMY_ID]
        xposs = [DUMMY_ID]
        heads = [DUMMY_ID]
        feats = [DUMMY_ID]
        miscs = [DUMMY_ID]
        lemma_scripts = [DUMMY_ID]
        deprels = [DUMMY_ID]
        skipped_tokens = 0

        for n_token, token in enumerate(sequence["treeJson"]["nodesJson"].values()):
            # TODO: why?
            if not isinstance(token["ID"], str):
                skipped_tokens += 1
                continue

            token_len = input.tokens_len[n_token + 1 - skipped_tokens]
            token_padding = [DUMMY_ID] * (token_len - 1)

            upos = [
                self.annotation_schema.encode_upos(token["UPOS"], token["FORM"])
            ] + token_padding
            xpos = [
                self.annotation_schema.encode_xpos(token["XPOS"], token["FORM"])
            ] + token_padding
            feat = [
                self.annotation_schema.encode_feats(token["FEATS"], token["FORM"])
            ] + token_padding
            misc = [
                self.annotation_schema.encode_misc(token["MISC"], token["FORM"])
            ] + token_padding
            lemma_script = [
                self.annotation_schema.encode_lemma_script(
                    token["FORM"], token["LEMMA"]
                )
            ] + token_padding

            # Becomes 0 for the root token
            head = [sum(input.tokens_len[: token["HEAD"]])] + token_padding
            deprel = token["DEPREL"]

            deprel = [
                self.annotation_schema.encode_deprel(deprel, token["FORM"])
            ] + token_padding
            # Example of what we have for a token of 2 subtokens
            # form = ["eat", "ing"]
            # pos = [4, DUMMY_ID]
            # head = [2, DUMMY_ID]
            # lemma_script = [3424, DUMMY_ID]
            # token_len = 2
            uposs += upos
            xposs += xpos
            heads += head
            deprels += deprel
            feats += feat
            miscs += misc
            lemma_scripts += lemma_script

        return SequenceTraining_T(
            pred_data=input,
            uposs=uposs,
            xposs=xposs,
            heads=heads,
            deprels=deprels,
            feats=feats,
            miscs=miscs,
            lemma_scripts=lemma_scripts,
        )

    def _get_processed(
        self, sentence_json: sentenceJson_T, idx: int
    ) -> SequenceTraining_T:
        pred_data = self._get_input(sentence_json, idx)
        return self._get_output(sentence_json, pred_data)

    def collate_predict(self, sentences: List[Sequence_T]) -> SequencePredictionBatch_T:
        # Add padding values so that the entire batch has the same length, then collect
        # the field tensors for all sequences into a single tensor for each field.
        max_sentence_length = max(
            [len(sentence.sequence_token_ids) for sentence in sentences]
        )
        tok_starts_word_batch = tensor(
            [
                self._pad_list(sentence.tok_starts_word, False, max_sentence_length)
                for sentence in sentences
            ]
        )
        idx_converter_batch = tensor(
            [
                self._pad_list(sentence.idx_converter, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        idx_batch = tensor([sentence.idx for sentence in sentences])
        # The docs say to pad with the PAD token and just mask those, but for some
        # reason we get better performance when we pad with CLS and mask those
        # (including the leading CLS).
        seq_ids_batch = tensor(
            [
                self._pad_list(
                    sentence.sequence_token_ids, self.CLS_token_id, max_sentence_length
                )
                for sentence in sentences
            ]
        )
        attn_masks = (seq_ids_batch != self.CLS_token_id).long()

        return SequencePredictionBatch_T(
            idx=idx_batch,
            sequence_token_ids=seq_ids_batch,
            tok_starts_word=tok_starts_word_batch,
            attn_masks=attn_masks,
            idx_converter=idx_converter_batch,
            max_sentence_length=max_sentence_length,
        )

    def collate_train(
        self, sentences: List[SequenceTraining_T]
    ) -> SequenceTrainingBatch_T:
        batch_prediction_data = self.collate_predict(sentences)
        max_sentence_length = batch_prediction_data.max_sentence_length

        uposs_batch = tensor(
            [
                self._pad_list(sentence.uposs, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        xposs_batch = tensor(
            [
                self._pad_list(sentence.xposs, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        heads_batch = tensor(
            [
                self._pad_list(sentence.heads, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        deprels_batch = tensor(
            [
                self._pad_list(sentence.deprels, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        feats_batch = tensor(
            [
                self._pad_list(sentence.feats, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        miscs_batch = tensor(
            [
                self._pad_list(sentence.miscs, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )
        lemma_scripts_batch = tensor(
            [
                self._pad_list(sentence.lemma_scripts, DUMMY_ID, max_sentence_length)
                for sentence in sentences
            ]
        )

        return SequenceTrainingBatch_T(
            batch_prediction_data,
            uposs=uposs_batch,
            xposs=xposs_batch,
            heads=heads_batch,
            deprels=deprels_batch,
            feats=feats_batch,
            miscs=miscs_batch,
            lemma_scripts=lemma_scripts_batch,
        )

    def construct_sentence_prediction(
        self,
        idx,
        uposs_preds: List[int] = [],
        xposs_preds: List[int] = [],
        chuliu_heads: List[int] = [],
        deprels_pred_chulius: List[int] = [],
        feats_preds: List[int] = [],
        miscs_preds: List[int] = [],
        lemma_scripts_preds: List[int] = [],
        partial_pred_config=PartialPredictionConfig(),
    ) -> sentenceJson_T:
        """Constructs the final sentence structure prediction by overwriting the model's
        predictions with the input data where specified. The metadata is copied as well,
        since it is not predicted.
        """
        predicted_sentence: sentenceJson_T = self.sequences[idx].sentence_json.copy()
        tokens = list(predicted_sentence["treeJson"]["nodesJson"].values())
        annotation_schema = self.annotation_schema

        # For each of the predicted fields, we overwrite the value copied from the input
        # with the predicted value if configured to do so.
        # MISC is a special case as it can be parially predicted (lot of miscs are info
        # that are unpredictable or not relevant to be predicted)
        for n_token, token in enumerate(tokens):
            if partial_pred_config.keep_upos == "NONE" or (
                partial_pred_config.keep_upos == "EXISTING"
                and token["UPOS"] == CONLLU_BLANK
            ):
                token["UPOS"] = annotation_schema.uposs[uposs_preds[n_token]]

            if partial_pred_config.keep_xpos == "NONE" or (
                partial_pred_config.keep_xpos == "EXISTING"
                and token["XPOS"] == CONLLU_BLANK
            ):
                token["XPOS"] = annotation_schema.xposs[xposs_preds[n_token]]

            if partial_pred_config.keep_heads == "NONE" or (
                partial_pred_config.keep_heads == "EXISTING"
                and token["HEAD"] == DUMMY_ID
            ):
                # this one is special as for keep_heads == "EXISTING", we already
                # handled the case earlier in the code
                token["HEAD"] = chuliu_heads[n_token]

            if partial_pred_config.keep_deprels == "NONE" or (
                partial_pred_config.keep_deprels == "EXISTING"
                and token["DEPREL"] == CONLLU_BLANK
            ):
                token["DEPREL"] = annotation_schema.deprels[
                    deprels_pred_chulius[n_token]
                ]

            if partial_pred_config.keep_feats == "NONE" or (
                partial_pred_config.keep_feats == "EXISTING" and token["FEATS"] == {}
            ):
                token["FEATS"] = _featuresConllToJson(
                    annotation_schema.feats[feats_preds[n_token]]
                )

            if partial_pred_config.keep_miscs == "NONE" or (
                partial_pred_config.keep_miscs == "EXISTING"
            ):
                miscs_to_rewrite = _featuresConllToJson(
                    annotation_schema.miscs[miscs_preds[n_token]]
                )
                for key, value in miscs_to_rewrite.items():
                    token["MISC"][key] = value

            if partial_pred_config.keep_lemmas == "NONE" or (
                partial_pred_config.keep_lemmas == "EXISTING"
                and token["LEMMA"] == CONLLU_BLANK
            ):
                lemma_script = annotation_schema.lemma_scripts[
                    lemma_scripts_preds[n_token]
                ]
                token["LEMMA"] = apply_lemma_rule(token["FORM"], lemma_script)
        return predicted_sentence

    # TODO: This suggests to me that we shouldn't actually have separate predict/train
    # data classes. It's weird to have to refer back to the JSON here.
    # TODO: this exists solely to construct the argument to
    # chuliu_edmonds_one_root_with_constraints. Move it closer to that usage (inside
    # that function or in its (only) caller).
    def get_constrained_dependency_for_chuliu(self, idx: int) -> List[Tuple]:
        """
        idx: index of the sentence in the dataset
        Returns a list of tuples (i, j), indicating that the ith word is
        dependent on the jth word in sentence idx."""
        forced_relations: List[Tuple] = []

        sentence_json: sentenceJson_T = self.sequences[idx].sentence_json
        for token_json in sentence_json["treeJson"]["nodesJson"].values():
            if token_json["HEAD"] >= 0:
                forced_relations.append((int(token_json["ID"]), token_json["HEAD"]))

        return forced_relations
